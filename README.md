# **ğŸš€ OllamaApiFacadeJS - For Express.js with LangChainJS**

**OllamaApiFacadeJS** is an open-source **Node.js** library designed to seamlessly integrate an **Express.js backend** with the **Ollama API** using **LangChainJS**. This allows clients expecting an Ollama-compatible backend - such as [Open WebUI](https://github.com/open-webui/open-webui) - to interact with your **Express.js API** effortlessly.

It serves as a **Node.js counterpart** to the [**.NET-based OllamaApiFacade**](https://github.com/GregorBiswanger/OllamaApiFacade), providing a similar level of integration but optimized for the JavaScript/TypeScript ecosystem.

## **âœ¨ Features**

âœ… **Ollama-Compatible API for Express.js** â€“ Easily expose your Express backend as an Ollama API.  
âœ… **Supports Local AI Models (e.g., LM Studio)** â€“ Works with local inference engines like **LM Studio**.  
âœ… **Seamless Integration with LangChainJS** â€“ Enables natural language processing with LangChainJS.  
âœ… **Streaming Support** â€“ Stream AI-generated responses directly to clients.  
âœ… **Custom Model Names** â€“ Configure custom model names for full flexibility.  
âœ… **Optimized for TypeScript** â€“ Includes full TypeScript support (`.d.ts` files) for better IntelliSense.  

## **ğŸ“¦ Installation**

You can install OllamaApiFacadeJS via NPM or PNPM:

```sh
pnpm add ollama-api-facade-js
```

or

```sh
npm install ollama-api-facade-js
```

## **ğŸ›  Getting Started**

### **1ï¸âƒ£ Prerequisites**

- **Node.js 18+**
- **Express.js**
- **LangChainJS**
- **LM Studio or another local LLM provider** (optional)

### **2ï¸âƒ£ Basic Express.js Example**

Hereâ€™s how to integrate OllamaApiFacadeJS into an **Express.js application**:

```ts
import express from 'express';
import { ChatOpenAI } from '@langchain/openai';
import createOllamaApiFacade from 'ollama-api-facade-js';

const chatOpenAI = new ChatOpenAI({
  apiKey: 'none',
  configuration: {
    baseURL: 'http://localhost:1234/v1', // LM Studio Endpoint
  },
});

const app = express();
const ollamaApi = createOllamaApiFacade(app, chatOpenAI);

ollamaApi.postApiChat(async (chatRequest, chatModel, chatResponse) => {
    const result = await chatModel.invoke(chatRequest.messages);
    chatResponse.asStream(result);
});

ollamaApi.listen();
```

ğŸ“Œ **What does this setup do?**

- **Creates an Express.js server.**
- **Initializes the Ollama API facade using LangChainJS.**
- **Handles AI chat requests with streaming responses.**
- **Starts the server on `http://localhost:11434` (default Ollama port).**

## **ğŸ“¡ Running Open WebUI with Docker**

After setting up your **Express.js backend**, you can integrate it with **Open WebUI** by running:

```sh
docker run -d -p 8080:8080 --add-host=host.docker.internal:host-gateway --name open-webui ghcr.io/open-webui/open-webui:main
```

â¡ Open WebUI will now be accessible at:  
**<http://localhost:8080>**  

For advanced configurations (e.g., GPU support), refer to the official [Open WebUI GitHub repo](https://github.com/open-webui/open-webui).

## **ğŸ·ï¸ Customizing Model Names**

By default, the API uses the model name `"nodeapi"`. To specify a **custom model name**, pass it as an argument:

```ts
const ollamaApi = createOllamaApiFacade(app, chatOpenAI, "my-custom-model");
```

## **ğŸ“¡ Streaming AI Responses**

OllamaApiFacadeJS supports **streaming responses** to **improve response times** and **user experience**:

```ts
ollamaApi.postApiChat(async (chatRequest, chatModel, chatResponse) => {
    const result = await chatModel.stream(chatRequest.messages);
    chatResponse.asStream(result); // Handles both streams & single responses
});
```

ğŸ’¡ **Automatically detects whether streaming is supported** and adapts accordingly.

## **ğŸ Debugging & Logging**

To debug request/response communication, enable **JSON request logging** with the built-in middleware:

```ts
import { jsonRequestLogger } from 'ollama-api-facade-js/middlewares';

app.use(jsonRequestLogger);
```

ğŸ“Œ **Logs Request & Response Data**  
âœ… HTTP Method & URL  
âœ… Request Headers & Body  
âœ… Response Status & Body  

This helps **troubleshoot API interactions** easily.

## **ğŸ¤ Contributing**

We welcome **contributions** from the community!  
To contribute:

1. **Fork** the repo.
2. **Create a branch** (`feature/new-feature`).
3. **Commit your changes** & **push the branch**.
4. **Submit a pull request** for review.

## **ğŸ“„ License**

This project is licensed under the **MIT License**.

ğŸ’¡ **Created by Gregor Biswanger â€“ Microsoft MVP for Azure AI & Web App Development**.

## **ğŸ™ Acknowledgments**

- [LangChainJS](https://js.langchain.com/)
- [Open WebUI](https://openwebui.com)
- [LM Studio](https://lmstudio.ai/)

### **ğŸš€ Ready to build your AI-powered Express.js backend? Get started today!**

If you have questions, feel free to **open an issue** on GitHub.  

## **ğŸ”¥ Summary**

âœ… **This README follows best practices**  
âœ… **Clear structure with installation, usage, and advanced setup**  
âœ… **Code snippets are formatted & easy to follow**  
âœ… **Includes streaming, debugging, and customization options**  
âœ… **Encourages contributions & community engagement**  

Let me know if you need any refinements! ğŸš€ğŸ”¥
